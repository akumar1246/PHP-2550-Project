---
title: "High-risk Sources and Seasons of C. Jejuni Outbreaks. Methods and Analysis Plan"
author: "Nataliya Kyrychenko, Anusha Kumar, Kyla Finlayson"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Install packages
  # If returns "dependency packages needed" please install those packages before
  # trying to install "DMwR"
#install.packages("~/Desktop/DMwR_0.4.1.tar.gz", repos = NULL, type = "source")

# Load in libraries
library(DiagrammeR)
library(aod)
library(broom)
library(eHOF)
library(glmtoolbox)
library(lmtest)
library(boot)
library(DMwR)
library(caret)
library(InformationValue)
library(pROC)
library(tidyverse)
library(kableExtra)
library(rsvg)
library(DiagrammeRsvg)
library(lme4)
library(glmnet)
library(mice)
library(DescTools)

# Load in cleaned data
isolates <- read.csv("~/Desktop/isolates_clean.csv", na.strings=c("","NA"))
```

**Question of Interest**

The initial question of our project was: "What sources of infection and times of year have a high likelihood of seeing an outbreak?." After our literature review and exploratory data analysis, the initial question remains the same as previously. We will be aiming to create a best-fitting model using the variables we have within our dataset. As a result, we may find that other variables that have a statistically significant relationship with Campylobacter jejuni outbreaks. While season and source of infection will be our primary outcomes of interest, we will also report any other significant relationships we uncover in the results section.

**Data Pre-Processing**

We will mostly be keeping the same data pre-processing method as we used in our first draft. For final analysis we will use only observations with complete data.

According to the comments received from the first draft literature review and exploratory data analysis, a flow chart was added to demonstrate how the final dataset was created. Moreover, for the final paper the first draft will be revised and shortened. Below is the flowchart we will be adding to our project:

<br>
<br>
<br>
```{r, out.width="50%", out.height="50%", fig.cap = "Exclusion/Inclusion Criteria Flowchart", fig.align= "center"}
 # **Subject to change depending on missing data method**
  
data <- list(a=80114, b=74654, c=58198, d=43089)

  # First and second lines set our visualization as an "undirected graph" definition
  # Third line creates a dot layout with 5 nodes in it
  # Fourth line assigns a label to each node
  # Fifth line connects nodes
plot <- grViz("digraph graph1 {
  
graph [layout = dot]

# node definitions with substituted label text
node [shape = rectangle, height=0.2, fillcolor = Biege, fontsize=8, width=2]
a [label = '@@1']
b [label = '@@2']
c [label = '@@3']
d [label = '@@4']

a -> b -> c -> d [arrowsize=0.3, arrowtail=0.1, minlen=1]

}

[1]:  paste0('Raw Data (n = ', data$a, ' obs.)')
[2]: paste0('Remove observations before 2012 (n = ', data$b, ' obs.)')
[3]: paste0('Remove observations after 2020 (n = ', data$c, ' obs.)')
[4]: paste0('Remove observations outside of USA (n = ', data$d, ' obs.)')
")

plot %>% export_svg %>% charToRaw %>% rsvg_png("~/Desktop/graph.png")
knitr::include_graphics("~/Desktop/graph.png")
```
<br>
<br>
<br>

After our initial implementation below, we decided we are going to group *Month* variable into seasons and average temperature per month per state that would be important for searching patterns between climate and outbreaks moving forward.

**Methods**

The dependent variable "Outbreak" of the study is binary: 0 - no outbreak, 1 - outbreak. To build a prediction model for the outcome of interest, we will be using logistic regression. The key independent variables are *Source*, which is a categorical variable, and *Month*, which is a discrete numerical variable. Both of the variables will be included in the final model. In addition, considering the importance of temperature, husbandry laws, and general state-level differences, we will be incorporating state into our model structure as well. Considering that strains are grouped into states, fixed effects logistic regression will be used. We will create a logistic fixed effects model in R using a binomial family argument with the link function of logit, and fixed effects for state. The significance level will be set at 0.05.

First, we will describe our data and outline our data pre-processing and filtering methods as we did in the first draft of our project submission. Then, we will add a table called Table 1 outlining the descriptive statistics of our final dataset. We will also include descriptive statistics stratified by our outcome of interest, *Outbreak*, in Table 1.

Next, we will deal with our missing data. Rather than throw out a large chunk of our data due to 1 missing variable (State, 35% missing), we will code our NA values for State as "Other". We will do the same for our other variable with missing values, SNP.cluster, which has about 16% of its values missing. Then, we will perform multiple imputation excluding the variables State and SNP.cluster because they have more than 50 categories. Variables with more than 50 categories cannot be used with multiple imputation and will not add much information to the predictions.

The analysis will be conducted in R (RStudio version 2022.07.2). For the analysis the following R packages will be used: (1) aod package is aimed to conduct analysis of overdispersed data, and function for generalized linear mixed effect models are available, (2) broom package allows to organize summary of models into tidy tables, (3) lmerTest package - for p-values of our mixed effects models based on the Satterthwaite approximation, (4) glmtoolbox - for Hesmer-Lemeshow goodness of fit test (hltest), (5) boot - for bootstrapping technique (boot), (7) UBL - for conducting informed undersampling using the CNN method, (8) DMwR - for conducting informed oversampling via SMOTE, and potentially more which we will add as needed. The "DMwR" package is no longer available on the CRAN repository, as a result, it will be installed via the archived versions of these packages which can be found under the "packages" folder in the project Github page. 

The model building process will begin by first balancing our data. Only approximately 7% of our isolate observations are linked to an outbreak, therefore our groups are very unequal. To balance them, we will instead be using either oversampling, using the synthetic minority oversampling technique (SMOTE), or weighting, where weights are defined as the inverse of the proportion of observations in our data. We originally also attempted using undersampling with the condensed nearest neighbors (CNN) rule, but found it was too computationally inefficient for our data. We will test both methods to see which results in the best accuracy, as measured by AUC, sensitivity, and specificity. We will do this by first splitting our data into a training set and a testing set, randomly sampling 70% of our data as the training set and the remaining 30% as our testing set. Then, we will apply both methods to the training data and run a logistic mixed effects model with only our two primary predictors of interest (*Source* and *Season*), as well as fixed effects for State. Then, we will compare our predicted results with our test set of data and look at the measures mentioned earlier to see which method performed better. We will summarize these measures for each method in two figures: one figure called Figure 1 which shows the AUC curves for each method, and one figure called Figure 2 which shows the calibration plots for each method.

After we have decided on either using weighting or oversampling, we will implement it to balance our data, assess the balance, and then begin the model building process. We will refer to our exploratory data analysis and decide on a final list of predictor variables we should potentially include in our model. Then, we will build models using all of the potential variables and determine which model fit will be best for our data. The models we will fit include: a logistic regression model, a fixed effects logistic regression model with fixed effects for state, a fixed effects logistic regression model with fixed effects for state and year, a mixed effects logistic regression model with fixed effects and random intercepts for state, a mixed effects logistic regression model with fixed effects for year and state and random intercept for state, and lastly, a mixed effects logistic regression model with fixed effects and random intercepts for state and year. We will determine which model is best-fitting by looking at performance measures like AUC, accuracy, specificity, sensitivity, and calibration plots. Then, once we find the best-fitting model, we will perform variable selection using stepwise backward selection based on AIC. We will include both variables *Source* and *Month* in our final model regardless of whether or not they are found to be significant. If one or both are found to not be significant, then we are still able to answer our question at interest. Then, once we have our final main effects model, we will consider interactions using the same process. We will build a model using all interactions of our filtered-down main effects variables, and run a backwards stepwise selection process based on AIC again to find our final model. Once we have our final model, we will display the results in a table called Table 2, showing both the exponentiated estimates of the coefficients, p-values, standard error, and other model measures for both our regular coefficients as well as our fixed effects.

Then, we will check the fit of our final model using our validation data set. Calibration and discrimination of the model will be checked using measures of accuracy including AUC, specificity, and sensitivity, and measures of calibration including Brier score and callibration plots. We will display these measures in both figures and/or tables.

Lastly, we will check the assumptions of our model. First and foremost, one key assumption includes linearity. In our logistic regression model, a linear relationship should exist between the dependent and independent variables. When examining the error terms, or residuals, there does not have to be a normal distribution of residuals. Additionally, we do not require homoscedasticity (homogeneity of variances). We can also observe that for logistic regression, the dependent variable does not use the ratio or interval scale (Assumptions of Logistic Regression, n.d.). More particularly, when implementing logistic regression methods, we make the assumption of a binary outcome (some examples include yes vs. no, positive vs. negative, 1 vs. 0, etc.). In our project, our binary outcome is represented by our outbreak variable (1 if outbreak, 0 if no outbreak). The linear relationship explains the association between the logit of the outcome and the dependent (or predictor) variables. The logit function is **logit(p) = log(p/(1-p))**. Within this logit function, p represents the outcome probabilities. We also observe that within the predictor variables, there is no multicollinearity (known as high intercorrelations). Within our predictors, we also do not have any extreme values, influential values, or outliers (Logistic Regression Assumptions and Diagnostics in R - Articles - STHDA, n.d.).

It is important that we can check our assumptions for logistic regression. When assessing the dependent variable, it is easy to assess if it is binary or dichotomous through a quick glance, as the dependent variable would have to fall into one of two clearly defined categories. When checking multicollinearity, we are aware that there should be no multicollinearity (or very little multicollinearity) amongst our independent variables, which can be justified by examining the VIF. The VIF between the predictor variables should not be very high, which can also be determined through certain tests, such as "Spearman's rank correlation coefficient" or the "Pearson correlation coefficient." As logistic regression generally needs quite large sample sizes, we can examine our sample size through a quick glance. More larger sample sizes produce more powerful results in our statistical analysis.

**Justification**

*Training set and validation set*
The internal validation of the model is planned, thus the dataset needs to be randomly divided into training and test samples. The train dataset is described as data used for model fitting purposes. This is known as the actual dataset, used to train the model (when examining the case with neural networks, we can refer to weights and biases). The model refers to this set and learns from it (Shah, 2017).

Through the validation dataset, the model fit provides us with an unbiased evaluation through the dataset we train. The validation set helps us make evaluations and make small adjustments to any model parameters. While the model looks at this data, it never learns from the validation dataset. As a result, the validation set can indirectly affect the model. We can refer to the validation set as the "development set" (Shah, 2017).

The results from the validation set help make adjustments to higher-level hyperparameters. We should also note that if we have do not have many hyperparameters in our models, we can easily validate and fine-tune them. However, with more prevalent hyperparamters, a larger validation set is required to compensate (Shah, 2017).

*Missing variables*

Our dataset has several key variables with missing values. While *Source* variable has very few missing values, *State* variable has 35% missingness. Moreover, *State*  is nominal categorical variable and has 50 levels that restricts possibilities of multiple imputation. 
We need to conduct complete case analysis that will bias the results of our study within current dataset and is a serious limitation of our research. Thus the results of current analysis can be considered only as hypothesis generating results.

*Oversampling or undersampling*

We will need to balance our data because we do not want our logistic mixed effects model to predict all of our observations to the majority class, "No outbreak". In our case, it may not make sense to use propensity score weighting in this application, as there is unlikely to be selection bias on our "treatment" variables *Month* and *Source* or variables our dataset that are strongly related to them. Other methods we could have potentially used are weighting or changing the threshold probability, which we may explore in the final implementation of our project. However, the most work has been done in sampling approaches, therefore we decided to try this approach. If we use undersampling, we will be undersampling the majority class, which are isolates not linked to an outbreak, by identifying the most useful observations. If we use oversampling, we will be oversampling the minority class, which are isolates linked to an outbreak, by interpolating between points in our minority class. Both methods are very efficient for balancing data, and once our data is balanced we will be able to create a more accurate regression model.

*Logistic fixed effects model*

We will be using a logistic model because our outcome is binary, and we want to be able to predict the risk of of our outcome, an outbreak, happening based on our other covariates. We will be using a fixed effects model because we have observations that vary by state, and we are interested in determining the overall effect of month and isolation source on an outbreak adjusting for state-level differences.

*Bootstrapping*

The bootstrap technique will be used because it is the most reliable way to validate a model. It is less prone to overfitting and can be used with any dataset, no matter how large or small. This is done by randomly dividing the dataset into two different components—training and test samples. The training sample will be used to train the model, while the test sample will be used to evaluate how accurate it is at making predictions on new data.

**Initial Implementation**

First, we make sure that all of our variables are of the correct structure. Then, we will remove all observations with missing data. Lastly, we will remove any variables that we don't want to potentially include in our final model.

```{r, echo=FALSE}
  # Convert variables to factor
isolates$Isolation.type <- as.factor(isolates$Isolation.type)
isolates$Year <- as.factor(isolates$Year)
isolates$Month <- as.factor(isolates$Month)
isolates$Source <- as.factor(isolates$Source)
isolates$Outbreak <- as.factor(isolates$Outbreak)
isolates$Season <- as.factor(isolates$Season)
```


```{r}
# Multiple imputation
set.seed(4)

mice_out <- mice(isolates, 5, pri=F, 
                 predictorMatrix = quickpred(isolates, 
                                             include = c("Isolation.type",
                                                         "Year",
                                                         "Month",
                                                         "Source",
                                                         "Outbreak",
                                                         "Season"),
                     # Excluding these variables because they have 50+ categories
                                             exclude = c("State",
                                                         "SNP.cluster")))

imputed_isolates <- mice::complete(mice_out,action="long") 

```


```{r, echo=FALSE}
#  Convert to factor

imputed_isolates$SNP.cluster <- as.factor(imputed_isolates$SNP.cluster)
imputed_isolates$State <- as.factor(imputed_isolates$State)
```

```{r, echo=FALSE}
  # Remove variables we won't be using in the model, subject to change

imputed_isolates <- imputed_isolates %>%
  select(-c(X, Min.same, Min.diff, Isolate.identifiers, Isolate, YearMonth,
            AMR.genotypes, Strain, Month, .imp, .id))
```

We randomly split our data into two sets, one with 70% of our observations (n=19387) which is our training set, and the remaining 30% of observations (n=8309) in our testing set. After trying to implement CNN undersampling using the UBL package in R, we found that this method was too computationally inefficient for our large dataset. As a result, we will instead compare the balancing method of weighting with the balancing method of SMOTE. First, we will generate weights for each of our observations that are equal to the inverse of the number of samples we have for each category (linked to outbreak or not linked to outbreak) for our training data. Next for the SMOTE method, we balance our training data using the SMOTE function from the DMwR package and save the resulting dataset. Next, we create two logistic fixed effects regression models-- one using our dataset resulting from applying SMOTE to our training data, and the other implementing weights on our training data. Both models have source and month as predictors, as well as fixed effects for state. Then, we apply both models on our test dataset and compare performance measures.

```{r, echo=FALSE}
# Randomly split into test and train set

  #Set seed so it's the same split every time
set.seed(77)

  #Create a random 70-30 split using sample()
split <- sample(nrow(imputed_isolates), nrow(imputed_isolates)*.7)

  #All observations in 'split" are train, all that are not in split are test
train <- imputed_isolates[split,]
test <- imputed_isolates[-split,]
```


```{r, echo=FALSE}
# Find best oversampling and undersampling parameters to use for SMOTE

  #Set seed
set.seed(7)

SMOTE_params <- function(df) { 
  #' Runs 10-fold CV for SMOTE parameters
  #' @param df, data set
  #' @return auc, list of AUC for each model
  # SMOTE
  aucs <- c()
  
  for (i in seq(100, 2000, 100)){
    for (j in seq(100, 1000, 100)){
      
  SMOTE_data <- SMOTE(Outbreak ~., 
                      df, 
                      perc.over = i, 
                      perc.under = j)
  model <- glm(Outbreak ~ Source + Season,
            data = SMOTE_data,
            family = binomial(link = "logit"))
  
  preds <- predict(model, SMOTE_data, type = "response")
  auc <- paste(i, j, auc(SMOTE_data$Outbreak, preds))
  aucs <- c(auc, aucs)
   }
  }
  return(aucs)
} 
```

```{r}
aucs <- SMOTE_params(train)
```

```{r, echo=FALSE}
# Weighting
train$weights <- ifelse(train$Outbreak == 1, 1/0.045, 1/0.955)
test$weights <- ifelse(test$Outbreak == 1, 1/0.045, 1/0.955)
```

```{r, echo=FALSE}
set.seed(7)

  # Convert to factor to use for SMOTE
train$Outbreak <- as.factor(train$Outbreak)

# Oversampling (SMOTE)

  # perc.over = decides how many extra cases from the minority class are generated
  # k = number of nearest neighbors used
  # perc.under = decides how many extra cases from the majority class are generated

SMOTE_data <- SMOTE(Outbreak ~., train, perc.over = 1200, k = 3, perc.under = 200)
```

```{r, echo=FALSE, include=FALSE}
  # Check proportions after SMOTE
table(SMOTE_data$Outbreak)
```

```{r, echo=FALSE}

# Create logistic regression model using each balancing method
  # Weighting
m1 <- glm(Outbreak ~ Source + Season,
            data = train,
            family = binomial(link = "logit"),
            weights = weights)
  #Smote
m2 <- glm(Outbreak ~ Source + Season,
            data = SMOTE_data,
            family = binomial(link = "logit"))
```

```{r, echo=FALSE, include=FALSE}
#Expand levels that may not be in training set
m1$xlevels[["State"]] <- union(m1$xlevels[["State"]], levels(train$State))

# Collect predicted values for each model
preds_weight_tr <- predict(m1, train, type = "response")
preds_SMOTE_tr <- predict(m2, SMOTE_data, type = "response")

# Gather measures of performance for both models using predictions
confusionMatrix(train$Outbreak, preds_weight_tr)
confusionMatrix(SMOTE_data$Outbreak, preds_SMOTE_tr)

auc(train$Outbreak, preds_weight_tr)
auc(SMOTE_data$Outbreak, preds_SMOTE_tr)
```


```{r, echo=FALSE}
performance_measures <- data.frame(accuracy = c(round((73513+5739)/150811, 2), 
                                                round((122835+54843)/235949, 2)),
                                    sensitivity = c(round(5739/(5739+70921), 2),
                                                    round(54843/(54843+30213), 2)),
                                    specificity = c(round(73513/(72513+638), 2),
                                                    round(122835/(122835+28058),2)),
                                    auc = c(0.756, 0.8186))

rownames(performance_measures) <- c("Weighting Method", "SMOTE Method")
colnames(performance_measures) <- c("Accuracy", "Sensitivity","Specificity", "AUC")
```

```{r, echo=FALSE}
performance_measures %>%
  kable() %>%
  kable_styling(latex_options = c("hold_position")) %>%
  add_header_above(c("Table 1: Performance Measures for Balancing Methods"=5))
```

Looking at Table 1, we see that while the SMOTE method has better accuracy, sensitivity and overall AUC. While the weighting method has better specificity, the model is naive. It seems to predict that the majority of isolates are not linked to an outbreak, without being able to distinguish which are linked. As a result, we will be continuing with the oversampled data.

Next, we will be trying a variety of different model types, and then we will be comparing their performance. We will use the same variables for each model, using the full model without interactions, and then will perform variable selection afterwards.

```{r}
# Making sure our variables are factors
SMOTE_data$Isolation.type <- as.factor(SMOTE_data$Isolation.type)
SMOTE_data$Year <- as.factor(SMOTE_data$Year)
SMOTE_data$Source <- as.factor(SMOTE_data$Source)
SMOTE_data$Outbreak <- as.factor(SMOTE_data$Outbreak)
SMOTE_data$Season <- as.factor(SMOTE_data$Season)
```

```{r}
  # Model fitting: Simple logistic regression

model1 <- glm(Outbreak ~ Source + Season + Isolation.type + temperature,
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Set same levels so predictions work
model1$xlevels[["State"]] <- union(model1$xlevels[["State"]], 
                                   levels(SMOTE_data$State))

# Collect predicted values for each model
preds1 <- predict(model1, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds1)
auc(SMOTE_data$Outbreak, preds1)
```

```{r, echo=FALSE, warning = FALSE}
  # Model fitting: Logistic regression with fixed effects for state

model2 <- glm(Outbreak ~ Source + Season + Isolation.type +
                temperature + (State - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Set same levels so predictions work
model2$xlevels[["State"]] <- union(model2$xlevels[["State"]], 
                                   levels(SMOTE_data$State))

# Collect predicted values for each model
preds2 <- predict(model2, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds2)
auc(SMOTE_data$Outbreak, preds2)
```

```{r, warning=FALSE}
  # Model fitting: Logistic regression with random effects for SNP.cluster

model3 <- glm(Outbreak ~ Source + Season + Isolation.type +
                temperature + (State - 1) + (Year - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds3 <- predict(model3, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds3)
auc(SMOTE_data$Outbreak, preds3)
```


```{r, echo=FALSE, warning = FALSE}
  # Model fitting: Logistic regression with fixed effects for state and year

model4 <- glmer(Outbreak ~ Source + Season + Isolation.type + 
                temperature + (1 | State),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds4 <- predict(model4, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds4)
auc(SMOTE_data$Outbreak, preds4)
```


```{r, echo=FALSE, warning = FALSE}
  # Model fitting: Logistic regression with fixed effects for state and year

model5 <- glmer(Outbreak ~ Source + Season + Isolation.type + 
                temperature + (1 | State) + (1 | Year),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds5 <- predict(model5, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds5)
auc(SMOTE_data$Outbreak, preds5)
```


Notes:
- Our AUC is the same for our model whether we have fixed and random effects for state and year, or we have just fixed effects for state and year. As a result, we will be choosing the model with fixed effects for state and year.

```{r, warning=FALSE}
# Variable Selection for main effects

model6 <-  glm(Outbreak ~ temperature + Source + Season + 
                 (State - 1) + (Year - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds6 <- predict(model6, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds6)
auc(SMOTE_data$Outbreak, preds6)
```


```{r, warning=FALSE}
# Variable Selection for main effects

model7 <- glm(Outbreak ~ Source + Season + (State - 1) + (Year - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds7 <- predict(model7, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds7)
auc(SMOTE_data$Outbreak, preds7)
```

Notes:
- Best-fitting model includes all main effects

```{r, warning=FALSE}
# Variable Selection for interactions

model8 <- glm(Outbreak ~ Source*Season + temperature + (State - 1) + (Year - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds8 <- predict(model8, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds8)
auc(SMOTE_data$Outbreak, preds8)
```

```{r, warning=FALSE}
# Variable Selection for interactions

model9 <- glm(Outbreak ~ Source*Season*Isolation.type + temperature + 
                (State - 1) + (Year - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds9 <- predict(model9, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds9)
auc(SMOTE_data$Outbreak, preds9)
```


Notes:
- Model performance is not significantly improving by adding interactions, and adding interactions will hinder the interpretability of our model. As a result, we will not add any.

```{r, echo=FALSE, include = FALSE}
# Performance for training set

model6$xlevels[["State"]] <- union(model6$xlevels[["State"]], 
                                   levels(SMOTE_data$State))

# Collect predicted values for training set
preds_final1 <- predict(model6, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds_final1)
auc(SMOTE_data$Outbreak, preds_final1)

# Performance for validation set
model3$xlevels[["State"]] <- union(test$xlevels[["State"]], 
                                   levels(test$State))

# Collect predicted values for each model
preds_final2 <- predict(model6, test, type = "response")

confusionMatrix(test$Outbreak, preds_final2)
auc(test$Outbreak, preds_final2)

# Measures
performance_measures2 <- data.frame(accuracy = c(round((140515+68363)/235949, 2), 
                                                round((56642+1784)/64634, 2)),
                                    sensitivity = c(round(68363/(68363+12533), 2),
                                                    round(1784/(1784+5119), 2)),
                                    specificity = c(round(140515/(140515+14538), 2),
                                                    round(56642/(56642+1089),2)),
                                    auc = c(0.952, 0.896))

rownames(performance_measures2) <- c("Training Data", "Validation Data")

colnames(performance_measures2) <- c("Accuracy", "Sensitivity", "Specificity", "AUC")
```

```{r, echo=FALSE}
performance_measures2 %>%
  kable() %>%
  kable_styling(latex_options = c("hold_position")) %>%
  add_header_above(c("Table 2: Performance Measures for Model Types"=5))
```

```{r}
summary(model6)
```

Notes on model:
- AUC of 0.952 on training data and 0.896 on validation data (not too overfit) and 0.90 accuracy
- Cons: low sensitivity, but still not horrible considering how unbalanced the original data is
- Season of spring is significant at 0.05 level and isolates that are collected in spring are exp(-0.602238) = 0.5476 times (or 45.24%) less likely to be linked to an outbreak than isolates that are collected in fall.
- Season of summer is significant at 0.05 level and isolates that are collected in spring are exp(0.449940) = 1.5682 times (or 56.82%) more likely to be linked to an outbreak than isolates that are collected in fall.
- Season of winter is significant at 0.05 level and isolates that are collected in winter are exp(-1.106360) = 0.3308 times (or 66.92%) less likely to be linked to an outbreak than isolates that are collected in fall.
- Temperature is significant at 0.05 level: for each one degree (in Celsius) increase in average temperature, an isolate is exp(0.008885) = 1.008925 times (or 0.89%) more likely to be linked to an outbreak. 
- Source is not significantly associated with outbreaks.


```{r, include=FALSE}

# Create dataframe with observed vs predicted probabilities
cal_data <- with(test, data.frame(y = ifelse(test$Outbreak == "1", 1, 0),
                                        prob = predict(model6, test, 
                                                       type = "response")))

# Create dataframe with observed vs predicted probabilities
cal_data2 <- with(SMOTE_data, data.frame(y = ifelse(SMOTE_data$Outbreak == "1", 1, 0),
                                        prob = predict(model6, SMOTE_data, 
                                                       type = "response")))

BrierScore(cal_data$y, cal_data$prob)
BrierScore(cal_data2$y, cal_data2$prob)
```

```{r}
roc_LASSO <- roc(cal_data$y, cal_data$prob)

f1 <- ggroc(roc_LASSO) +
  labs(x = "Specificity", y = "Sensitivity",
       title = "Validation Set") +
  geom_text(aes(x = 0.25, y = 0.25, 
           label = "AUC: 0.952",
           size = 3)) +
  geom_text(aes(x = 0.25, y = 0.2, 
           label = "Accuracy: 0.89",
           size = 3)) +
  geom_text(aes(x = 0.25, y = 0.15, 
           label = "Brier Score: 0.083",
           size = 3)) +
  theme(legend.position = "none")
  
roc_LASSO2 <- roc(cal_data2$y, cal_data2$prob)

f2 <- ggroc(roc_LASSO2) +
  labs(x = "Specificity", y = "Sensitivity",
       title = "Training Set") +
  geom_text(aes(x = 0.25, y = 0.25, 
           label = "AUC: 0.897",
           size = 3)) +
  geom_text(aes(x = 0.25, y = 0.2, 
           label = "Accuracy: 0.90",
           size = 3)) +
  geom_text(aes(x = 0.25, y = 0.15, 
           label = "Brier Score: 0.072",
           size = 3)) +
  theme(legend.position = "none")
```

```{r}
cal_data2$y <- factor(cal_data2$y, 
                     labels = c("No Outbreak", "Outbreak"),
                     levels = c(0, 1))
f1 <- mplot_roc(cal_data2$y, cal_data2$prob, subtitle = "Training Data")
f3 <- mplot_density(cal_data2$y, cal_data2$prob, subtitle = "Training Data")

cal_data$y <- factor(cal_data$y, 
                     labels = c("No Outbreak", "Outbreak"),
                     levels = c(0, 1))
f2 <- mplot_roc(cal_data$y, cal_data$prob, subtitle = "Validation Data")
f4 <- mplot_density(cal_data$y, cal_data$prob, subtitle =  "Validation Data")
```

```{r, fig.cap = "ROC Curves: Training vs. Validation", fig.height=3}

  # Display plots side-by-side
grid.arrange(f1, f2, nrow=1)

f3
f4
```

```{r}
# Try cross-validation to avoid overfitting

lasso <- function(df) { 
  #' Runs 10-fold CV for lasso and returns corresponding coefficients 
  #' @param df, data set
  #' @return coef, coefficients for minimum cv error
  
  # Matrix form for ordered variables 
  x.ord <- model.matrix(Outbreak ~ Source + Season + temperature +
                          (State - 1) + (Year - 1), data = df)[,-1] 
  y.ord <- df$Outbreak 
  
  # Generate folds
  k <- 10 
  set.seed(1) # consistent seeds between imputed data sets
  folds <- sample(1:k, nrow(df), replace=TRUE)
  
  # Lasso model
  lasso_mod <- cv.glmnet(x.ord, y.ord, nfolds = 10, foldid = folds, 
                         alpha = 1, family = "binomial") 
  
  # Get coefficients 
  coef <- coef(lasso_mod, lambda = lasso_mod$lambda.min) 
  return(coef) 
} 

# Find predicted probabilities on training data
coefs <- lasso(SMOTE_data)
x_vars <- model.matrix(Outbreak ~ Source + Season + temperature +
                          (State - 1) + (Year - 1), data = SMOTE_data)
SMOTE_data$score <- as.vector(x_vars %*% coefs)
mod <- glm(Outbreak ~score, data = SMOTE_data, family = "binomial")
SMOTE_data$preds <- predict(mod, type="response")

# Performance measures of training data
confusionMatrix(SMOTE_data$Outbreak, SMOTE_data$preds)
auc(SMOTE_data$Outbreak, SMOTE_data$preds)

# Find predicted probabilities on test data
x_vars <- model.matrix(Outbreak ~ Source + Season + temperature +
                          (State - 1) + (Year - 1), data = test)
test$score <- as.vector(x_vars %*% coefs)
test$preds <- predict(mod, test, type="response")

# Performance measures on test data
confusionMatrix(test$Outbreak, test$preds)
auc(test$Outbreak, test$preds)

# Overfitting is worse using this
```


**References**

1.) Assumptions of Logistic Regression. (n.d.). Statistics Solutions. https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/

2.) Logistic Regression Assumptions and Diagnostics in R - Articles - STHDA. (n.d.). Www.sthda.com. http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/

3.) Shah, T. (2017, December 6). About Train, Validation and Test Sets in Machine Learning. Towards Data Science; Towards Data Science. https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7

4.) What is Logistic Regression? (n.d.). Careerfoundry.com. https://careerfoundry.com/en/blog/data-analytics/what-is-logistic-regression/
