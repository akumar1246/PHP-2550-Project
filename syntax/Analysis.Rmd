---
title: "High-risk Sources and Seasons of C. Jejuni Outbreaks. Methods and Analysis Plan"
author: "Nataliya Kyrychenko, Anusha Kumar, Kyla Finlayson"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Install packages
  # If returns "dependency packages needed" please install those packages before
  # trying to install "DMwR"
#install.packages("~/Desktop/DMwR_0.4.1.tar.gz", repos = NULL, type = "source")

# Load in libraries
library(DiagrammeR)
library(aod)
library(broom)
library(eHOF)
library(glmtoolbox)
library(lmtest)
library(boot)
library(DMwR)
library(caret)
library(InformationValue)
library(pROC)
library(tidyverse)
library(kableExtra)
library(rsvg)
library(DiagrammeRsvg)
library(lme4)
library(glmnet)
library(mice)
library(DescTools)

# Load in cleaned data
isolates <- read.csv("~/Desktop/isolates_clean.csv", na.strings=c("","NA"))
```

```{r, echo=FALSE}
  # Convert variables to factor
isolates$Isolation.type <- as.factor(isolates$Isolation.type)
isolates$Year <- as.factor(isolates$Year)
isolates$Month <- as.factor(isolates$Month)
isolates$Source <- as.factor(isolates$Source)
isolates$Outbreak <- as.factor(isolates$Outbreak)
isolates$Season <- as.factor(isolates$Season)
```


```{r}
# Multiple imputation
set.seed(4)

mice_out <- mice(isolates, 5, pri=F, 
                 predictorMatrix = quickpred(isolates, 
                                             include = c("Isolation.type",
                                                         "Year",
                                                         "Month",
                                                         "Source",
                                                         "Outbreak",
                                                         "Season"),
                     # Excluding these variables because they have 50+ categories
                                             exclude = c("State",
                                                         "SNP.cluster")))

imputed_isolates <- mice::complete(mice_out,action="long") 

```


```{r, echo=FALSE}
#  Convert to factor

imputed_isolates$SNP.cluster <- as.factor(imputed_isolates$SNP.cluster)
imputed_isolates$State <- as.factor(imputed_isolates$State)
```

```{r, echo=FALSE}
  # Remove variables we won't be using in the model

imputed_isolates <- imputed_isolates %>%
  select(-c(X, Min.same, Min.diff, Isolate.identifiers, Isolate, YearMonth,
            AMR.genotypes, Strain, Month, .imp, .id))
```

```{r, echo=FALSE}
# Randomly split into test and train set

  #Set seed so it's the same split every time
set.seed(77)

  #Create a random 70-30 split using sample()
split <- sample(nrow(imputed_isolates), nrow(imputed_isolates)*.7)

  #All observations in 'split" are train, all that are not in split are test
train <- imputed_isolates[split,]
test <- imputed_isolates[-split,]
```

```{r, echo=FALSE}
# Find best oversampling and undersampling parameters to use for SMOTE

  #Set seed
set.seed(7)

SMOTE_params <- function(df) { 
  #' Runs 10-fold CV for SMOTE parameters
  #' @param df, data set
  #' @return auc, list of AUC for each model
  # SMOTE
  aucs <- c()
  
  for (i in seq(100, 2000, 100)){
    for (j in seq(100, 1000, 100)){
      
  SMOTE_data <- SMOTE(Outbreak ~., 
                      df, 
                      perc.over = i, 
                      perc.under = j)
  model <- glm(Outbreak ~ Source + Season,
            data = SMOTE_data,
            family = binomial(link = "logit"))
  
  preds <- predict(model, SMOTE_data, type = "response")
  auc <- paste(i, j, auc(SMOTE_data$Outbreak, preds))
  aucs <- c(auc, aucs)
   }
  }
  return(aucs)
} 
```

```{r}
# Run our custom SMOTE parameter finding function on our training data
aucs <- SMOTE_params(train)
```

```{r, echo=FALSE}
# Apply weights for weighting method
train$weights <- ifelse(train$Outbreak == 1, 1/0.045, 1/0.955)
test$weights <- ifelse(test$Outbreak == 1, 1/0.045, 1/0.955)
```

```{r, echo=FALSE}
set.seed(7)

  # Convert to factor to use for SMOTE
train$Outbreak <- as.factor(train$Outbreak)

# Oversampling (SMOTE)

  # perc.over = decides how many extra cases from the minority class are generated
  # k = number of nearest neighbors used
  # perc.under = decides how many extra cases from the majority class are generated

SMOTE_data <- SMOTE(Outbreak ~., train, perc.over = 1200, k = 3, perc.under = 200)
```

```{r, echo=FALSE, include=FALSE}
  # Check proportions after SMOTE
table(SMOTE_data$Outbreak)
```

```{r, echo=FALSE}

# Create logistic regression model using each balancing method
  # Weighting
m1 <- glm(Outbreak ~ Source + Season,
            data = train,
            family = binomial(link = "logit"),
            weights = weights)
  #Smote
m2 <- glm(Outbreak ~ Source + Season,
            data = SMOTE_data,
            family = binomial(link = "logit"))
```

```{r, echo=FALSE, include=FALSE}
#Expand levels that may not be in training set
m1$xlevels[["State"]] <- union(m1$xlevels[["State"]], levels(train$State))

# Collect predicted values for each model
preds_weight_tr <- predict(m1, train, type = "response")
preds_SMOTE_tr <- predict(m2, SMOTE_data, type = "response")

# Gather measures of performance for both models using predictions
confusionMatrix(train$Outbreak, preds_weight_tr)
confusionMatrix(SMOTE_data$Outbreak, preds_SMOTE_tr)

auc(train$Outbreak, preds_weight_tr)
auc(SMOTE_data$Outbreak, preds_SMOTE_tr)
```


```{r, echo=FALSE}
performance_measures <- data.frame(accuracy = c(round((73513+5739)/150811, 2), 
                                                round((122835+54843)/235949, 2)),
                                    sensitivity = c(round(5739/(5739+70921), 2),
                                                    round(54843/(54843+30213), 2)),
                                    specificity = c(round(73513/(72513+638), 2),
                                                    round(122835/(122835+28058),2)),
                                    auc = c(0.756, 0.8186))

rownames(performance_measures) <- c("Weighting Method", "SMOTE Method")
colnames(performance_measures) <- c("Accuracy", "Sensitivity","Specificity", "AUC")
```

```{r, echo=FALSE}
performance_measures %>%
  kable() %>%
  kable_styling(latex_options = c("hold_position")) %>%
  add_header_above(c("Table 1: Performance Measures for Balancing Methods"=5))
```

Looking at Table 1, we see that while the SMOTE method has better accuracy, sensitivity and overall AUC. While the weighting method has better specificity, the model is naive. It seems to predict that the majority of isolates are not linked to an outbreak, without being able to distinguish which are linked. As a result, we will be continuing with the oversampled data.

Next, we will be trying a variety of different model types, and then we will be comparing their performance. We will use the same variables for each model, using the full model without interactions, and then will perform variable selection afterwards.

```{r}
# Making sure our variables are factors
SMOTE_data$Isolation.type <- as.factor(SMOTE_data$Isolation.type)
SMOTE_data$Year <- as.factor(SMOTE_data$Year)
SMOTE_data$Source <- as.factor(SMOTE_data$Source)
SMOTE_data$Outbreak <- as.factor(SMOTE_data$Outbreak)
SMOTE_data$Season <- as.factor(SMOTE_data$Season)
```

```{r}
  # Model fitting: Simple logistic regression

model1 <- glm(Outbreak ~ Source + Season + Isolation.type + temperature,
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Set same levels so predictions work
model1$xlevels[["State"]] <- union(model1$xlevels[["State"]], 
                                   levels(SMOTE_data$State))

# Collect predicted values for each model
preds1 <- predict(model1, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds1)
auc(SMOTE_data$Outbreak, preds1)
```

```{r, echo=FALSE, warning = FALSE}
  # Model fitting: Logistic regression with fixed effects for state

model2 <- glm(Outbreak ~ Source + Season + Isolation.type +
                temperature + (State - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Set same levels so predictions work
model2$xlevels[["State"]] <- union(model2$xlevels[["State"]], 
                                   levels(SMOTE_data$State))

# Collect predicted values for each model
preds2 <- predict(model2, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds2)
auc(SMOTE_data$Outbreak, preds2)
```

```{r, warning=FALSE}
  # Model fitting: Logistic regression with random effects for SNP.cluster

model3 <- glm(Outbreak ~ Source + Season + Isolation.type +
                temperature + (State - 1) + (Year - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds3 <- predict(model3, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds3)
auc(SMOTE_data$Outbreak, preds3)
```


```{r, echo=FALSE, warning = FALSE}
  # Model fitting: Logistic regression with fixed effects for state and year

model4 <- glmer(Outbreak ~ Source + Season + Isolation.type + 
                temperature + (1 | State),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds4 <- predict(model4, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds4)
auc(SMOTE_data$Outbreak, preds4)
```


```{r, echo=FALSE, warning = FALSE}
  # Model fitting: Logistic regression with fixed effects for state and year

model5 <- glmer(Outbreak ~ Source + Season + Isolation.type + 
                temperature + (1 | State) + (1 | Year),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds5 <- predict(model5, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds5)
auc(SMOTE_data$Outbreak, preds5)
```

Notes:
- Our AUC is the same for our model whether we have fixed and random effects for state and year, or we have just fixed effects for state and year. As a result, we will be choosing the model with fixed effects for state and year.

```{r, warning=FALSE}
# Variable Selection for main effects

model6 <-  glm(Outbreak ~ temperature + Source + Season + 
                 (State - 1) + (Year - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds6 <- predict(model6, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds6)
auc(SMOTE_data$Outbreak, preds6)
```


```{r, warning=FALSE}
# Variable Selection for main effects

model7 <- glm(Outbreak ~ Source + Season + (State - 1) + (Year - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds7 <- predict(model7, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds7)
auc(SMOTE_data$Outbreak, preds7)
```

Notes:
- Best-fitting model includes all main effects

```{r, warning=FALSE}
# Variable Selection for interactions

model8 <- glm(Outbreak ~ Source*Season + temperature + (State - 1) + (Year - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds8 <- predict(model8, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds8)
auc(SMOTE_data$Outbreak, preds8)
```

```{r, warning=FALSE}
# Variable Selection for interactions

model9 <- glm(Outbreak ~ Source*Season*Isolation.type + temperature + 
                (State - 1) + (Year - 1),
            data = SMOTE_data,
            family = binomial(link = "logit"))

# Collect predicted values for each model
preds9 <- predict(model9, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds9)
auc(SMOTE_data$Outbreak, preds9)
```


Notes:
- Model performance is not significantly improving by adding interactions, and adding interactions will hinder the interpretability of our model. As a result, we will not add any.

```{r, echo=FALSE, include = FALSE}
# Performance for training set

model6$xlevels[["State"]] <- union(model6$xlevels[["State"]], 
                                   levels(SMOTE_data$State))

# Collect predicted values for training set
preds_final1 <- predict(model6, SMOTE_data, type = "response")

confusionMatrix(SMOTE_data$Outbreak, preds_final1)
auc(SMOTE_data$Outbreak, preds_final1)

# Performance for validation set
model3$xlevels[["State"]] <- union(test$xlevels[["State"]], 
                                   levels(test$State))

# Collect predicted values for each model
preds_final2 <- predict(model6, test, type = "response")

confusionMatrix(test$Outbreak, preds_final2)
auc(test$Outbreak, preds_final2)

# Measures
performance_measures2 <- data.frame(accuracy = c(round((140515+68363)/235949, 2), 
                                                round((56642+1784)/64634, 2)),
                                    sensitivity = c(round(68363/(68363+12533), 2),
                                                    round(1784/(1784+5119), 2)),
                                    specificity = c(round(140515/(140515+14538), 2),
                                                    round(56642/(56642+1089),2)),
                                    auc = c(0.952, 0.896))

rownames(performance_measures2) <- c("Training Data", "Validation Data")

colnames(performance_measures2) <- c("Accuracy", "Sensitivity", "Specificity", "AUC")
```

```{r, echo=FALSE}
performance_measures2 %>%
  kable() %>%
  kable_styling(latex_options = c("hold_position")) %>%
  add_header_above(c("Table 2: Performance Measures for Model Types"=5))
```

```{r}
summary(model6)
```

Notes on model:
- AUC of 0.952 on training data and 0.896 on validation data (not too overfit) and 0.90 accuracy
- Cons: low sensitivity, but still not horrible considering how unbalanced the original data is
- Season of spring is significant at 0.05 level and isolates that are collected in spring are exp(-0.602238) = 0.5476 times (or 45.24%) less likely to be linked to an outbreak than isolates that are collected in fall.
- Season of summer is significant at 0.05 level and isolates that are collected in spring are exp(0.449940) = 1.5682 times (or 56.82%) more likely to be linked to an outbreak than isolates that are collected in fall.
- Season of winter is significant at 0.05 level and isolates that are collected in winter are exp(-1.106360) = 0.3308 times (or 66.92%) less likely to be linked to an outbreak than isolates that are collected in fall.
- Temperature is significant at 0.05 level: for each one degree (in Celsius) increase in average temperature, an isolate is exp(0.008885) = 1.008925 times (or 0.89%) more likely to be linked to an outbreak. 
- Source is not significantly associated with outbreaks.

```{r, include=FALSE}

# Create dataframe with observed vs predicted probabilities
cal_data <- with(test, data.frame(y = ifelse(test$Outbreak == "1", 1, 0),
                                        prob = predict(model6, test, 
                                                       type = "response")))

# Create dataframe with observed vs predicted probabilities
cal_data2 <- with(SMOTE_data, data.frame(y = ifelse(SMOTE_data$Outbreak == "1", 1, 0),
                                        prob = predict(model6, SMOTE_data, 
                                                       type = "response")))

BrierScore(cal_data$y, cal_data$prob)
BrierScore(cal_data2$y, cal_data2$prob)
```

```{r}
roc_LASSO <- roc(cal_data$y, cal_data$prob)

f1 <- ggroc(roc_LASSO) +
  labs(x = "Specificity", y = "Sensitivity",
       title = "Validation Set") +
  geom_text(aes(x = 0.25, y = 0.25, 
           label = "AUC: 0.952",
           size = 3)) +
  geom_text(aes(x = 0.25, y = 0.2, 
           label = "Accuracy: 0.89",
           size = 3)) +
  geom_text(aes(x = 0.25, y = 0.15, 
           label = "Brier Score: 0.083",
           size = 3)) +
  theme(legend.position = "none")
  
roc_LASSO2 <- roc(cal_data2$y, cal_data2$prob)

f2 <- ggroc(roc_LASSO2) +
  labs(x = "Specificity", y = "Sensitivity",
       title = "Training Set") +
  geom_text(aes(x = 0.25, y = 0.25, 
           label = "AUC: 0.897",
           size = 3)) +
  geom_text(aes(x = 0.25, y = 0.2, 
           label = "Accuracy: 0.90",
           size = 3)) +
  geom_text(aes(x = 0.25, y = 0.15, 
           label = "Brier Score: 0.072",
           size = 3)) +
  theme(legend.position = "none")
```

```{r}
cal_data2$y <- factor(cal_data2$y, 
                     labels = c("No Outbreak", "Outbreak"),
                     levels = c(0, 1))
f1 <- mplot_roc(cal_data2$y, cal_data2$prob, subtitle = "Training Data")
f3 <- mplot_density(cal_data2$y, cal_data2$prob, subtitle = "Training Data")

cal_data$y <- factor(cal_data$y, 
                     labels = c("No Outbreak", "Outbreak"),
                     levels = c(0, 1))
f2 <- mplot_roc(cal_data$y, cal_data$prob, subtitle = "Validation Data")
f4 <- mplot_density(cal_data$y, cal_data$prob, subtitle =  "Validation Data")
```

```{r, fig.cap = "ROC Curves: Training vs. Validation", fig.height=3}

  # Display plots side-by-side
grid.arrange(f1, f2, nrow=1)

f3
f4
```

```{r}
# Try cross-validation to avoid overfitting

lasso <- function(df) { 
  #' Runs 10-fold CV for lasso and returns corresponding coefficients 
  #' @param df, data set
  #' @return coef, coefficients for minimum cv error
  
  # Matrix form for ordered variables 
  x.ord <- model.matrix(Outbreak ~ Source + Season + temperature +
                          (State - 1) + (Year - 1), data = df)[,-1] 
  y.ord <- df$Outbreak 
  
  # Generate folds
  k <- 10 
  set.seed(1) # consistent seeds between imputed data sets
  folds <- sample(1:k, nrow(df), replace=TRUE)
  
  # Lasso model
  lasso_mod <- cv.glmnet(x.ord, y.ord, nfolds = 10, foldid = folds, 
                         alpha = 1, family = "binomial") 
  
  # Get coefficients 
  coef <- coef(lasso_mod, lambda = lasso_mod$lambda.min) 
  return(coef) 
} 

# Find predicted probabilities on training data
coefs <- lasso(SMOTE_data)
x_vars <- model.matrix(Outbreak ~ Source + Season + temperature +
                          (State - 1) + (Year - 1), data = SMOTE_data)
SMOTE_data$score <- as.vector(x_vars %*% coefs)
mod <- glm(Outbreak ~score, data = SMOTE_data, family = "binomial")
SMOTE_data$preds <- predict(mod, type="response")

# Performance measures of training data
confusionMatrix(SMOTE_data$Outbreak, SMOTE_data$preds)
auc(SMOTE_data$Outbreak, SMOTE_data$preds)

# Find predicted probabilities on test data
x_vars <- model.matrix(Outbreak ~ Source + Season + temperature +
                          (State - 1) + (Year - 1), data = test)
test$score <- as.vector(x_vars %*% coefs)
test$preds <- predict(mod, test, type="response")

# Performance measures on test data
confusionMatrix(test$Outbreak, test$preds)
auc(test$Outbreak, test$preds)

# Overfitting is worse using this
```

## Check assumptions

```{r}
# checking multicollinearity
# variance inflation factor of model (VIF)
# output shows VIF for each of our predictor variables
VIF(model6)
```
```{r}
# checking linearity assumption
binnedplot(preds6, residuals(model6))
```

**References**

1.) Assumptions of Logistic Regression. (n.d.). Statistics Solutions. https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/

2.) Logistic Regression Assumptions and Diagnostics in R - Articles - STHDA. (n.d.). Www.sthda.com. http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/

3.) Shah, T. (2017, December 6). About Train, Validation and Test Sets in Machine Learning. Towards Data Science; Towards Data Science. https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7

4.) What is Logistic Regression? (n.d.). Careerfoundry.com. https://careerfoundry.com/en/blog/data-analytics/what-is-logistic-regression/
